{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"},{"sourceId":242592,"sourceType":"datasetVersion","datasetId":102285}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport json\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\ndata = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nMODEL_PATH = \"/kaggle/working/nn_weights.txt\"\n\ndata = np.array(data)\nm,n = data.shape\nnp.random.shuffle(data)\n\ndata_dev = data[0 :1000].T\nY_dev = data_dev[0]\nX_dev = data_dev[1:n] / 255\nnp.savez_compressed(\"/kaggle/working/dev.npz\", X_dev=X_dev, Y_dev=Y_dev)\n\ndata_train = data[1000 : m].T\nY_train = data_train[0]\nX_train = data_train[1:n] / 255\n\n    \n\ndef initParams():\n    w1 = np.random.randn(10, 784)\n    b1 = np.random.randn(10,1)\n    w2 = np.random.randn(10,10)\n    b2 = np.random.randn(10,1)\n    return w1, b1, w2, b2\n\ndef forwardProp(w1, w2, b1, b2, X):\n    Z1 = w1.dot(X) + b1\n    A1 = ReLU(Z1)\n    Z2 = w2.dot(A1) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n\ndef forwardPropTest(w1, w2, b1, b2, Xi):\n    Z1 = w1.dot(Xi) + b1\n    #print(Z1)\n    A1 = ReLU(Z1)\n    #print(A1)\n    Z2 = w2.dot(A1) + b2\n    #print(Z2)\n    A2 = softmax(Z2)\n    #print(A2)\n    print(A2[:, 0].argmax())\n    \n\ndef ReLU(Z):\n    return np.maximum(0,Z)\n\ndef softmax(Z, axis=0):              # columns are samples: (10, 1000)\n    Z = Z - Z.max(axis=axis, keepdims=True)   # stability\n    eZ = np.exp(Z)\n    return eZ / eZ.sum(axis=axis, keepdims=True)\n\n\n    \ndef updateParams(a, w1,dw1, b1, db1, w2, dw2, b2, db2):\n    w1 = w1 - a* dw1\n    b1 = b1 - a*db1\n    w2 = w2 - a* dw2\n    b2 = b2 - a* db2\n    return w1, w2, b1, b2\n\n\ndef oneHot(Y):\n    oneHotY = np.zeros((Y.size, Y.max() + 1), dtype=np.float32)\n    oneHotY[np.arange(Y.size), Y] = 1\n    oneHotY = oneHotY.T\n    return oneHotY\n\ndef backProp(w2, A2, X, Y, A1, Z1, Z2):\n    dZ2 = A2-oneHot(Y)\n    dW2 = (dZ2.dot(A1.T)) / m\n    dB2 = np.sum(dZ2) / m\n    gPrime = Z1 > 0\n    dZ1 = (w2.T.dot(dZ2)) * (gPrime)\n    dW1 = dZ1.dot(X.T) / m\n    dB1 = np.sum(dZ2) / m\n    return dB1, dB2, dW1, dW2\n\ndef get_Predictions(A2):\n    return np.argmax(A2, 0)\ndef checkAccuracy(predictions, Y):\n    \n    return np.sum(predictions == Y) / Y.size\n\ndef gradientDescent(X, Y, iterations, a):\n    w1, b1, w2, b2 = initParams()\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = forwardProp(w1,w2,b1,b2,X_train)\n        db1, db2, dw1, dw2 = backProp(w2, A2, X_train, Y_train, A1, Z1, Z2)\n        w1, w2, b1, b2 = updateParams(a, w1, dw1, b1, db1, w2, dw2, b2, db2)\n        if (i%10 == 0):\n            print(\"iteration: \", i)\n            print(\"Accuracy: \", checkAccuracy(get_Predictions(A2), Y))\n    return w1, b1, w2, b2\n    \n\ndef save_quadruple_txt(path, w1, b1, w2, b2):\n    \"\"\"\n    Save weights to a text (JSON) file.\n    Stores shape + dtype so you get the exact arrays back.\n    \"\"\"\n    def pack(arr):\n        arr = np.asarray(arr)\n        return {\"shape\": arr.shape, \"dtype\": str(arr.dtype), \"data\": arr.tolist()}\n\n    payload = {\n        \"w1\": pack(w1),\n        \"b1\": pack(b1),\n        \"w2\": pack(w2),\n        \"b2\": pack(b2),\n    }\n    with open(path, \"w\") as f:\n        json.dump(payload, f)  # add indent=2 for human-readable\n\n\ndef load_quadruple_txt(path):\n    \"\"\"\n    Load weights from the text (JSON) file created above.\n    Returns (w1, b1, w2, b2)\n    \"\"\"\n    def unpack(obj):\n        arr = np.array(obj[\"data\"], dtype=np.dtype(obj[\"dtype\"]))\n        return arr.reshape(obj[\"shape\"])\n\n    with open(path, \"r\") as f:\n        payload = json.load(f)\n\n    w1 = unpack(payload[\"w1\"])\n    b1 = unpack(payload[\"b1\"])\n    w2 = unpack(payload[\"w2\"])\n    b2 = unpack(payload[\"b2\"])\n    return w1, b1, w2, b2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:27:27.241529Z","iopub.execute_input":"2025-08-31T18:27:27.241868Z","iopub.status.idle":"2025-08-31T18:27:30.817933Z","shell.execute_reply.started":"2025-08-31T18:27:27.241842Z","shell.execute_reply":"2025-08-31T18:27:30.816917Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mnist-dataset/train-images.idx3-ubyte\n/kaggle/input/mnist-dataset/t10k-labels.idx1-ubyte\n/kaggle/input/mnist-dataset/t10k-images.idx3-ubyte\n/kaggle/input/mnist-dataset/train-labels.idx1-ubyte\n/kaggle/input/mnist-dataset/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\n/kaggle/input/mnist-dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\n/kaggle/input/mnist-dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte\n/kaggle/input/mnist-dataset/train-images-idx3-ubyte/train-images-idx3-ubyte\n/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nw1, b1, w2, b2 = gradientDescent(X_train, Y_train, 1000, 0.1)\n","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-08-31T02:48:45.957561Z","iopub.execute_input":"2025-08-31T02:48:45.958513Z","iopub.status.idle":"2025-08-31T02:49:58.580131Z","shell.execute_reply.started":"2025-08-31T02:48:45.958481Z","shell.execute_reply":"2025-08-31T02:49:58.579154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#save most recently trained model\nsave_quadruple_txt(MODEL_PATH, w1, b1, w2, b2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T02:50:11.627930Z","iopub.execute_input":"2025-08-31T02:50:11.628242Z","iopub.status.idle":"2025-08-31T02:50:11.645073Z","shell.execute_reply.started":"2025-08-31T02:50:11.628216Z","shell.execute_reply":"2025-08-31T02:50:11.644120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#load current model from txt file\nw1_, b1_, w2_, b2_ = load_quadruple_txt(MODEL_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:26:48.742155Z","iopub.execute_input":"2025-08-31T18:26:48.742502Z","iopub.status.idle":"2025-08-31T18:26:48.753192Z","shell.execute_reply.started":"2025-08-31T18:26:48.742476Z","shell.execute_reply":"2025-08-31T18:26:48.752134Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1230484816.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load current model from txt file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_quadruple_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'load_quadruple_txt' is not defined"],"ename":"NameError","evalue":"name 'load_quadruple_txt' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"#test *loaded model* on developmental data\nZ1, A1, Z2, A2 = forwardProp(w1_, w2_, b1_, b2_, X_dev)\nprint(checkAccuracy(get_Predictions(A2), Y_dev))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T02:54:55.760186Z","iopub.execute_input":"2025-08-31T02:54:55.760930Z","iopub.status.idle":"2025-08-31T02:54:55.767103Z","shell.execute_reply.started":"2025-08-31T02:54:55.760904Z","shell.execute_reply":"2025-08-31T02:54:55.766122Z"}},"outputs":[{"name":"stdout","text":"0.628\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#display random image in X_dev and then forward propogate using *loaded model*\ni = np.random.randint(1, 800) \nX_dev_transpose = X_dev.T\nx = X_dev_transpose[i]                 # shape (784,)\ny = Y_dev[i]\n\nimg = x.reshape(28, 28)        # -> (28,28)\nplt.imshow(img, cmap=\"gray\", vmin=0, vmax=1 if img.max()<=1.0 else 255)\nplt.title(f\"Label: {y}\")\nplt.axis(\"off\")\nplt.show()\n\n\n#test current model on this image\nforwardPropTest(w1_,w2_, b1_, b2_, X_dev[:, i:i+1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T02:50:44.008075Z","iopub.execute_input":"2025-08-31T02:50:44.008341Z","iopub.status.idle":"2025-08-31T02:50:44.253403Z","shell.execute_reply.started":"2025-08-31T02:50:44.008322Z","shell.execute_reply":"2025-08-31T02:50:44.252630Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}